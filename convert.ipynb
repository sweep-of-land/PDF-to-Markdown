{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "401a3d6a",
   "metadata": {},
   "source": [
    "## PDF to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b482699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å¤„ç†: 2026è®²ä¹‰ç­”æ¡ˆ.pdf -> 2026è®²ä¹‰ç­”æ¡ˆ_output.md\n",
      "æå–å®Œæˆ! ç« èŠ‚: 7, é¢˜ç›®: 115\n",
      "\n",
      "æ­£åœ¨å¤„ç†: 2026åˆ·é¢˜ç­è®²ä¹‰.pdf -> 2026åˆ·é¢˜ç­è®²ä¹‰_output.md\n",
      "æå–å®Œæˆ! ç« èŠ‚: 7, é¢˜ç›®: 115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import pymupdf  # PyMuPDF\n",
    "except ImportError:\n",
    "    print(\"é”™è¯¯: è¯·å…ˆå®‰è£… PyMuPDF åº“\")\n",
    "    print(\"å®‰è£…å‘½ä»¤: pip install pymupdf\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "def extract_page_number(text):\n",
    "    \"\"\"ä»é¡µé¢æ–‡æœ¬ä¸­æå–é¡µè„šé¡µç \"\"\"\n",
    "    pattern = r'-\\s*(\\d+)\\s*-'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if matches:\n",
    "        return matches[-1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def remove_page_numbers(text):\n",
    "    \"\"\"ç§»é™¤æ–‡æœ¬ä¸­çš„é¡µç \"\"\"\n",
    "    text = re.sub(r'-\\s*\\d+\\s*-', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_chapter_start(line):\n",
    "    \"\"\"åˆ¤æ–­æ˜¯å¦æ˜¯ç« èŠ‚å¼€å§‹\"\"\"\n",
    "    # åŒ¹é… \"ä¸“é¡¹è®­ç»ƒ\" å¼€å¤´\n",
    "    return line.strip().startswith(\"ä¸“é¡¹è®­ç»ƒ\")\n",
    "\n",
    "\n",
    "def is_question_start(line):\n",
    "    \"\"\"åˆ¤æ–­æ˜¯å¦æ˜¯é¢˜ç›®å¼€å§‹\"\"\"\n",
    "    # åŒ¹é…\"ç¬¬Xé¢˜\"ã€\"ç¬¬XXé¢˜\"ç­‰\n",
    "    pattern = r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+é¢˜'\n",
    "    return bool(re.match(pattern, line.strip()))\n",
    "\n",
    "\n",
    "def get_char_width_count(line):\n",
    "    \"\"\"è®¡ç®—å­—ç¬¦æ˜¾ç¤ºå®½åº¦ï¼ˆæ±‰å­—2ï¼Œè‹±æ–‡1ï¼Œè¿”å›æ€»å®½åº¦é™¤ä»¥2ï¼Œå³æŠ˜åˆæ±‰å­—æ•°ï¼‰\"\"\"\n",
    "    return sum(2 if ord(c) > 127 else 1 for c in line) / 2\n",
    "\n",
    "\n",
    "def merge_lines(lines):\n",
    "    \"\"\"\n",
    "    æ™ºèƒ½åˆå¹¶æ–‡æœ¬è¡Œ - æµå¼å¤„ç†ç‰ˆ\n",
    "    \n",
    "    é€»è¾‘ï¼š\n",
    "    1. éå†æ¯ä¸€è¡Œã€‚\n",
    "    2. æ£€æŸ¥\"ä¸Šä¸€è¡Œ\"æ˜¯å¦æ˜¯æ»¡è¡Œï¼ˆæ„å‘³ç€æ®µè½æœªç»“æŸï¼‰ã€‚\n",
    "    3. æ£€æŸ¥\"å½“å‰è¡Œ\"æ˜¯å¦æ˜¯åˆ—è¡¨é¡¹å¼€å¤´ï¼ˆæ„å‘³ç€å¼ºåˆ¶æ–°èµ·ä¸€è¡Œï¼‰ã€‚\n",
    "    4. å¦‚æœåˆ¤å®šä¸ºåŒä¸€æ®µè½ï¼Œåˆ™åˆå¹¶ï¼›å¦åˆ™ï¼Œå°†ä¸Šä¸€æ®µè½å­˜å…¥ç»“æœï¼Œå¼€å§‹æ–°æ®µè½ã€‚\n",
    "    \"\"\"\n",
    "    if not lines:\n",
    "        return \"\"\n",
    "    \n",
    "    # ç»“æœåˆ—è¡¨ï¼Œå­˜å‚¨å®Œæ•´çš„æ®µè½\n",
    "    paragraphs = []\n",
    "    \n",
    "    # å½“å‰æ­£åœ¨æ„å»ºçš„æ®µè½ç¼“å†²\n",
    "    current_buffer = \"\"\n",
    "    \n",
    "    # è®°å½•\"ä¸Šä¸€è¡ŒåŸå§‹æ–‡æœ¬\"çš„é•¿åº¦ç‰¹å¾ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦è‡ªç„¶æ¢è¡Œ\n",
    "    last_raw_line_width = 0\n",
    "    \n",
    "    # æ»¡è¡Œé˜ˆå€¼ï¼šå¦‚æœä¸€è¡Œè¶…è¿‡è¿™ä¸ªé•¿åº¦ï¼ˆæŠ˜åˆæ±‰å­—ï¼‰ï¼Œé€šå¸¸æ„å‘³ç€å®ƒæ˜¯æ®µè½çš„ä¸€éƒ¨åˆ†è€Œä¸æ˜¯ç»“å°¾\n",
    "    # ä¸€èˆ¬A4æ–‡æ¡£ä¸€è¡Œçº¦35-45ä¸ªæ±‰å­—ï¼Œè¿™é‡Œè®¾å®šä¸º35æ¯”è¾ƒå®‰å…¨\n",
    "    FULL_LINE_THRESHOLD = 35\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # è®¡ç®—å½“å‰è¡Œçš„ç‰¹å¾\n",
    "        # is_list_item = bool(re.match(r'^[\\(ï¼ˆ\\[ã€\\d1234567890ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼‰\\]ã€‘\\.]', line))\n",
    "        is_list_item = bool(re.match(\n",
    "            r'^('\n",
    "            r'[\\(ï¼ˆ\\[ã€\\d1234567890ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼‰\\]ã€‘\\.]'\n",
    "            r'|'\n",
    "            r'ææ–™[\\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+'  # ææ–™+æ•°å­—æˆ–ä¸­æ–‡æ•°å­—\n",
    "            r')', \n",
    "            line\n",
    "        ))\n",
    "        # å†³å®šæ˜¯ã€åˆå¹¶ã€‘è¿˜æ˜¯ã€æ–°èµ·ä¸€æ®µã€‘\n",
    "        # æ¡ä»¶1ï¼šå¦‚æœæ˜¯ç¬¬ä¸€è¡Œï¼Œç›´æ¥å…¥ç¼“å†²\n",
    "        if not current_buffer:\n",
    "            current_buffer = line\n",
    "            last_raw_line_width = get_char_width_count(line)\n",
    "            continue\n",
    "            \n",
    "        # æ¡ä»¶2ï¼šåˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶åˆ°ä¸Šä¸€æ®µ\n",
    "        # é€»è¾‘ï¼šå¦‚æœä¸Šä¸€è¡Œå¾ˆé•¿ï¼ˆæ¥è¿‘æ»¡è¡Œï¼‰ï¼Œä¸”å½“å‰è¡Œä¸æ˜¯æ˜æ˜¾çš„åˆ—è¡¨é¡¹ï¼ˆå¦‚\"1.\"ï¼‰ï¼Œåˆ™åˆå¹¶\n",
    "        should_merge = (last_raw_line_width >= FULL_LINE_THRESHOLD) and (not is_list_item)\n",
    "        \n",
    "        if should_merge:\n",
    "            # åˆå¹¶åˆ°å½“å‰ç¼“å†²ï¼ˆä¸åŠ æ¢è¡Œç¬¦ï¼Œæ±‰å­—ç›´æ¥æ‹¼æ¥ï¼‰\n",
    "            current_buffer += line\n",
    "        else:\n",
    "            # ç»“æŸä¸Šä¸€æ®µï¼Œå­˜å…¥ç»“æœ\n",
    "            paragraphs.append(current_buffer)\n",
    "            # å¼€å§‹æ–°çš„ä¸€æ®µ\n",
    "            current_buffer = line\n",
    "            \n",
    "        # æ›´æ–°ä¸Šä¸€è¡ŒåŸå§‹é•¿åº¦è®°å½•\n",
    "        last_raw_line_width = get_char_width_count(line)\n",
    "    \n",
    "    # å¾ªç¯ç»“æŸåï¼Œå¤„ç†ç¼“å†²åŒºä¸­æœ€åä¸€æ®µ\n",
    "    if current_buffer:\n",
    "        paragraphs.append(current_buffer)\n",
    "    \n",
    "    # Markdownä¸­ï¼Œæ®µè½ä¹‹é—´éœ€è¦ç©ºä¸€è¡Œï¼ˆå³ä¸¤ä¸ªæ¢è¡Œç¬¦ï¼‰\n",
    "    return \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "\n",
    "def process_pdf(pdf_path, output_path):\n",
    "    \"\"\"å¤„ç†PDFæ–‡ä»¶å¹¶è¾“å‡ºMarkdownæ ¼å¼\"\"\"\n",
    "    \n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    \n",
    "    chapters = []\n",
    "    current_chapter = None\n",
    "    current_question = None\n",
    "    current_page_num = None\n",
    "    \n",
    "    # ç”¨äºè·¨é¡µæ‹¼æ¥é¢˜ç›®æ ‡é¢˜çš„ä¸´æ—¶å˜é‡\n",
    "    pending_title_parts = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # æå–é¡µç \n",
    "        page_number = extract_page_number(text)\n",
    "        if page_number:\n",
    "            current_page_num = page_number\n",
    "        \n",
    "        clean_text = remove_page_numbers(text)\n",
    "        lines = clean_text.split('\\n')\n",
    "        \n",
    "        # é¢„å¤„ç†è¡Œï¼šå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œè¿‡æ»¤ç©ºè¡Œ\n",
    "        valid_lines = [line.strip() for line in lines if line.strip()]\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(valid_lines):\n",
    "            line = valid_lines[i]\n",
    "            \n",
    "            # 1. æ£€æŸ¥ç« èŠ‚å¼€å§‹\n",
    "            if is_chapter_start(line):\n",
    "                # ä¿å­˜æ—§æ•°æ®\n",
    "                if current_question and current_chapter:\n",
    "                    current_chapter['questions'].append(current_question)\n",
    "                if current_chapter:\n",
    "                    chapters.append(current_chapter)\n",
    "                \n",
    "                current_chapter = {\n",
    "                    'title': line,\n",
    "                    'questions': []\n",
    "                }\n",
    "                current_question = None\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # 2. æ£€æŸ¥é¢˜ç›®å¼€å§‹\n",
    "            if is_question_start(line):\n",
    "                # ä¿å­˜æ—§é¢˜ç›®\n",
    "                if current_question and current_chapter:\n",
    "                    current_chapter['questions'].append(current_question)\n",
    "                \n",
    "                # æ”¶é›†å¤šè¡Œæ ‡é¢˜\n",
    "                title_parts = [line]\n",
    "                j = i + 1\n",
    "                while j < len(valid_lines):\n",
    "                    next_line = valid_lines[j]\n",
    "                    # å¦‚æœä¸‹ä¸€è¡Œæ¯”è¾ƒé•¿ï¼Œæˆ–è€…ä¸æ˜¯æ˜æ˜¾çš„å†…å®¹æ®µè½ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜çš„å»¶ç»­\n",
    "                    # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„å‡è®¾ï¼šå¦‚æœä¸‹ä¸€è¡Œä¸åŒ…å«\"è¦æ±‚\"ã€\"ã€\"ä¸”ä¸Šä¸€è¡Œæ¯”è¾ƒé•¿ï¼Œåˆ™è§†ä¸ºæ ‡é¢˜å»¶ç»­\n",
    "                    prev_width = get_char_width_count(valid_lines[j-1])\n",
    "                    if prev_width >= 30 and not next_line.startswith('è¦æ±‚') and not next_line.startswith('ã€'):\n",
    "                         title_parts.append(next_line)\n",
    "                         j += 1\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                full_title = \"\".join(title_parts)\n",
    "                current_question = {\n",
    "                    'title': full_title,\n",
    "                    'page': current_page_num,\n",
    "                    'content': []\n",
    "                }\n",
    "                # æ›´æ–°ç´¢å¼•è·³è¿‡æ ‡é¢˜è¡Œ\n",
    "                i = j \n",
    "                continue\n",
    "            \n",
    "            # 3. æ™®é€šå†…å®¹\n",
    "            if current_question is not None:\n",
    "                current_question['content'].append(line)\n",
    "            \n",
    "            i += 1\n",
    "    \n",
    "    # å¾ªç¯ç»“æŸï¼Œä¿å­˜æœ€åä¸€é¡¹\n",
    "    if current_question and current_chapter:\n",
    "        current_chapter['questions'].append(current_question)\n",
    "    if current_chapter:\n",
    "        chapters.append(current_chapter)\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    # ç”ŸæˆMarkdownè¾“å‡º\n",
    "    markdown_output = []\n",
    "    \n",
    "    for chapter in chapters:\n",
    "        markdown_output.append(f\"# {chapter['title']}\\n\")\n",
    "        \n",
    "        for question in chapter['questions']:\n",
    "            markdown_output.append(f\"## {question['title']}\")\n",
    "            if question['page']:\n",
    "                markdown_output.append(f\"*ï¼ˆç¬¬ {question['page']} é¡µï¼‰*\\n\")\n",
    "            else:\n",
    "                markdown_output.append(\"\")\n",
    "            \n",
    "            # æ ¸å¿ƒä¿®æ”¹ï¼šå°†åˆ—è¡¨è¡Œä¸€æ¬¡æ€§ä¼ å…¥æ–°çš„ merge_lines å‡½æ•°\n",
    "            content = merge_lines(question['content'])\n",
    "            markdown_output.append(content)\n",
    "            markdown_output.append(\"\") # é¢˜ç›®é—´ç©ºè¡Œ\n",
    "    \n",
    "    output_text = '\\n'.join(markdown_output)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(output_text)\n",
    "    \n",
    "    return len(chapters), sum(len(ch['questions']) for ch in chapters)\n",
    "\n",
    "\n",
    "# --- æ‰§è¡Œéƒ¨åˆ† ---\n",
    "pdf_path_list = [\"2026è®²ä¹‰ç­”æ¡ˆ.pdf\", \"2026åˆ·é¢˜ç­è®²ä¹‰.pdf\"]\n",
    "\n",
    "for pdf_path in pdf_path_list:\n",
    "    if not Path(pdf_path).exists():\n",
    "        # print(f\"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}\") # è°ƒè¯•æ—¶ç”±äºæ–‡ä»¶ä¸åœ¨æœ¬åœ°å¯æ³¨é‡Š\n",
    "        continue\n",
    "\n",
    "    output_path = Path(pdf_path).stem + \"_output.md\"\n",
    "    print(f\"æ­£åœ¨å¤„ç†: {pdf_path} -> {output_path}\")\n",
    "\n",
    "    try:\n",
    "        chapter_count, question_count = process_pdf(pdf_path, output_path)\n",
    "        print(f\"æå–å®Œæˆ! ç« èŠ‚: {chapter_count}, é¢˜ç›®: {question_count}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç†å¼‚å¸¸: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b5820",
   "metadata": {},
   "source": [
    "## Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6beac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è§£æé¢˜ç›®å·...\n",
      "é¢˜ç›®å·è§£æå®Œæˆï¼Œå…± 7 ç« \n",
      "æ­£åœ¨è§£æç­”æ¡ˆå·...\n",
      "ç­”æ¡ˆå·è§£æå®Œæˆï¼Œå…± 7 ç« \n",
      "æ­£åœ¨åˆå¹¶...\n",
      "åˆå¹¶å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³: 2026åˆ·é¢˜ç­_é¢˜ç›®ä¸ç­”æ¡ˆåˆå¹¶ç‰ˆ.md\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def normalize_key(text):\n",
    "    \"\"\"\n",
    "    æ ‡å‡†åŒ–é”®å€¼ï¼šå»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ã€‚\n",
    "    ç”¨äºåœ¨ä¸¤ä¸ªæ–‡ä»¶ä¸­åŒ¹é…ç« èŠ‚å’Œé¢˜ç›®ï¼Œå¿½ç•¥æ ¼å¼å¾®å°å·®å¼‚ã€‚\n",
    "    \"\"\"\n",
    "    # å»é™¤ç©ºç™½å­—ç¬¦\n",
    "    text = \"\".join(text.split())\n",
    "    # å»é™¤å¸¸è§æ ‡ç‚¹ï¼ˆå¯æ ¹æ®éœ€è¦è¡¥å……ï¼‰\n",
    "    text = re.sub(r'[ã€ï¼Œã€‚ï¼š:ï¼ˆï¼‰\\(\\)\\[\\]ã€ã€‘\\.\\-]', '', text)\n",
    "    return text\n",
    "\n",
    "def parse_markdown(file_path):\n",
    "    \"\"\"\n",
    "    è§£æMarkdownæ–‡ä»¶ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®ã€‚\n",
    "    è¿”å›ç»“æ„: \n",
    "    [\n",
    "        {\n",
    "            \"title\": \"ç« èŠ‚æ ‡é¢˜\",\n",
    "            \"questions\": [\n",
    "                {\n",
    "                    \"title\": \"é¢˜ç›®æ ‡é¢˜\",\n",
    "                    \"page\": \"é¡µç å­—ç¬¦ä¸²\",\n",
    "                    \"content\": \"åŸå§‹å†…å®¹æ–‡æœ¬\"\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    if not Path(file_path).exists():\n",
    "        print(f\"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}\")\n",
    "        return []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    chapters = []\n",
    "    current_chapter = None\n",
    "    current_question = None\n",
    "    \n",
    "    # çŠ¶æ€æ ‡è®°\n",
    "    # content_buffer ç”¨äºå­˜å‚¨å½“å‰é¢˜ç›®ä¸‹çš„æ–‡æœ¬å†…å®¹\n",
    "    content_buffer = []\n",
    "\n",
    "    def save_current_question():\n",
    "        nonlocal current_question, content_buffer\n",
    "        if current_question:\n",
    "            current_question['content'] = \"\".join(content_buffer).strip()\n",
    "            if current_chapter:\n",
    "                current_chapter['questions'].append(current_question)\n",
    "            content_buffer = []\n",
    "            current_question = None\n",
    "\n",
    "    def save_current_chapter():\n",
    "        nonlocal current_chapter\n",
    "        if current_chapter:\n",
    "            save_current_question() # å…ˆä¿å­˜æ­£åœ¨å¤„ç†çš„é¢˜ç›®\n",
    "            chapters.append(current_chapter)\n",
    "            current_chapter = None\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # 1. è¯†åˆ«ç« èŠ‚ (# ç« èŠ‚å)\n",
    "        if line.startswith(\"# \"):\n",
    "            save_current_chapter()\n",
    "            current_chapter = {\n",
    "                \"title\": stripped.replace(\"# \", \"\").strip(),\n",
    "                \"questions\": []\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # 2. è¯†åˆ«é¢˜ç›® (## é¢˜ç›®å)\n",
    "        if line.startswith(\"## \"):\n",
    "            save_current_question()\n",
    "            current_question = {\n",
    "                \"title\": stripped.replace(\"## \", \"\").strip(),\n",
    "                \"page\": \"\",\n",
    "                \"content\": \"\"\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # 3. è¯†åˆ«é¡µç è¡Œ (*(ç¬¬ XX é¡µ)*)\n",
    "        # æ­£åˆ™åŒ¹é…å½¢å¦‚ *(ç¬¬ 113 é¡µ)* æˆ– *ï¼ˆç¬¬ 113 é¡µï¼‰*\n",
    "        # page_match = re.match(r'^\\s*[\\*]*[ï¼ˆ\\(]ç¬¬\\s*(\\d+)\\s*é¡µ[ï¼‰\\)][\\*]*', stripped)\n",
    "        page_match = re.search(r'[ï¼ˆ\\(ã€]ç¬¬\\s*(\\d+)\\s*é¡µ[ï¼‰\\)ã€‘]', stripped)\n",
    "        \n",
    "        if page_match and current_question:\n",
    "            current_question['page'] = page_match.group(1)\n",
    "            continue # é¡µç ä¸ä½œä¸ºæ­£æ–‡å†…å®¹å­˜å…¥buffer\n",
    "\n",
    "        # 4. æ™®é€šå†…å®¹\n",
    "        if current_question is not None:\n",
    "            content_buffer.append(line)\n",
    "\n",
    "    # å¾ªç¯ç»“æŸï¼Œä¿å­˜æœ€åçš„å‰©ä½™å†…å®¹\n",
    "    save_current_chapter()\n",
    "\n",
    "    return chapters\n",
    "\n",
    "def extract_answer_body(content):\n",
    "    \"\"\"\n",
    "    ä»ç­”æ¡ˆæ–‡æœ¬ä¸­æå–çœŸæ­£çš„ç­”æ¡ˆéƒ¨åˆ†ã€‚\n",
    "    é€šå¸¸ç­”æ¡ˆæ–‡ä»¶ä¼šé‡å¤é¢˜ç›®è¦æ±‚ï¼Œæˆ‘ä»¬åªéœ€è¦ã€å‚è€ƒç­”æ¡ˆã€‘ä¹‹åçš„éƒ¨åˆ†ã€‚\n",
    "    \"\"\"\n",
    "    if \"ã€å‚è€ƒç­”æ¡ˆã€‘\" in content:\n",
    "        parts = content.split(\"ã€å‚è€ƒç­”æ¡ˆã€‘\")\n",
    "        # è¿”å›æ ‡è®°åŠ ä¸Šåé¢çš„å†…å®¹\n",
    "        return \"ã€å‚è€ƒç­”æ¡ˆã€‘\\n\" + parts[1].strip()\n",
    "    else:\n",
    "        # å¦‚æœæ‰¾ä¸åˆ°æ ‡è®°ï¼Œå¯èƒ½æ˜¯ä¸€ä¸ªæ²¡æœ‰æ ‡å‡†æ ¼å¼çš„ç­”æ¡ˆï¼Œè¿”å›å…¨éƒ¨\n",
    "        return content\n",
    "\n",
    "def merge_markdowns(prob_file, ans_file, output_file):\n",
    "    print(\"æ­£åœ¨è§£æé¢˜ç›®å·...\")\n",
    "    prob_data = parse_markdown(prob_file)\n",
    "    print(f\"é¢˜ç›®å·è§£æå®Œæˆï¼Œå…± {len(prob_data)} ç« \")\n",
    "\n",
    "    print(\"æ­£åœ¨è§£æç­”æ¡ˆå·...\")\n",
    "    ans_data = parse_markdown(ans_file)\n",
    "    print(f\"ç­”æ¡ˆå·è§£æå®Œæˆï¼Œå…± {len(ans_data)} ç« \")\n",
    "\n",
    "    # æ„å»ºç­”æ¡ˆçš„å¿«é€ŸæŸ¥æ‰¾å­—å…¸\n",
    "    # ç»“æ„: { norm_chap_title: { norm_q_title: question_obj } }\n",
    "    ans_lookup = {}\n",
    "    for chap in ans_data:\n",
    "        c_key = normalize_key(chap['title'])\n",
    "        ans_lookup[c_key] = {}\n",
    "        for q in chap['questions']:\n",
    "            q_key = normalize_key(q['title'])\n",
    "            ans_lookup[c_key][q_key] = q\n",
    "\n",
    "    merged_lines = []\n",
    "\n",
    "    print(\"æ­£åœ¨åˆå¹¶...\")\n",
    "    \n",
    "    # ä»¥é¢˜ç›®å·ä¸ºåŸºå‡†è¿›è¡Œéå†\n",
    "    for chap in prob_data:\n",
    "        # å†™å…¥ç« èŠ‚æ ‡é¢˜\n",
    "        merged_lines.append(f\"# {chap['title']}\\n\")\n",
    "        \n",
    "        c_key = normalize_key(chap['title'])\n",
    "        ans_chap_dict = ans_lookup.get(c_key, {})\n",
    "\n",
    "        for q in chap['questions']:\n",
    "            q_title = q['title']\n",
    "            q_key = normalize_key(q_title)\n",
    "            \n",
    "            # æŸ¥æ‰¾å¯¹åº”çš„ç­”æ¡ˆ\n",
    "            ans_obj = ans_chap_dict.get(q_key)\n",
    "            \n",
    "            # 1. å†™å…¥é¢˜ç›®æ ‡é¢˜\n",
    "            merged_lines.append(f\"## {q_title}\")\n",
    "            \n",
    "            # 2. å†™å…¥é¡µç ä¿¡æ¯ (åˆå¹¶æ˜¾ç¤º)\n",
    "            p_page = q['page'] if q['page'] else \"æœªçŸ¥\"\n",
    "            a_page = ans_obj['page'] if (ans_obj and ans_obj['page']) else \"æœªæ‰¾åˆ°\"\n",
    "            \n",
    "            # ä½¿ç”¨å¼•ç”¨å—æˆ–æ–œä½“æ˜¾ç¤ºæ¥æºä¿¡æ¯\n",
    "            merged_lines.append(f\"\\n> ğŸ“ **é¢˜ç›®æ‰€åœ¨é¡µ**ï¼š{p_page}  |  ğŸ’¡ **ç­”æ¡ˆæ‰€åœ¨é¡µ**ï¼š{a_page}\\n\")\n",
    "            \n",
    "            # 3. å†™å…¥é¢˜ç›®ä¸ææ–™\n",
    "            merged_lines.append(\"### ğŸ“˜ é¢˜ç›®ä¸ææ–™\")\n",
    "            merged_lines.append(q['content'])\n",
    "            merged_lines.append(\"\") # ç©ºè¡Œ\n",
    "            \n",
    "            # 4. å†™å…¥å‚è€ƒç­”æ¡ˆ\n",
    "            merged_lines.append(\"### âœï¸ å‚è€ƒç­”æ¡ˆ\")\n",
    "            if ans_obj:\n",
    "                # æ™ºèƒ½æå–ï¼šåªå–ã€å‚è€ƒç­”æ¡ˆã€‘åçš„éƒ¨åˆ†ï¼Œé¿å…é‡å¤â€œè¦æ±‚â€\n",
    "                clean_ans = extract_answer_body(ans_obj['content'])\n",
    "                merged_lines.append(clean_ans)\n",
    "            else:\n",
    "                merged_lines.append(\"*ï¼ˆæœªåœ¨ç­”æ¡ˆæ–‡ä»¶ä¸­æ‰¾åˆ°å¯¹åº”é¢˜ç›®çš„ç­”æ¡ˆï¼Œè¯·æ£€æŸ¥æ ‡é¢˜æ˜¯å¦ä¸€è‡´ï¼‰*\")\n",
    "            \n",
    "            merged_lines.append(\"\\n---\\n\") # åˆ†éš”çº¿\n",
    "\n",
    "    # å†™å…¥æ–‡ä»¶\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(merged_lines))\n",
    "    \n",
    "    print(f\"åˆå¹¶å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³: {output_file}\")\n",
    "\n",
    "# --- é…ç½®ä¸æ‰§è¡Œ ---\n",
    "\n",
    "# ä½ çš„æ–‡ä»¶å\n",
    "problem_file = \"2026åˆ·é¢˜ç­è®²ä¹‰_output.md\"\n",
    "answer_file = \"2026è®²ä¹‰ç­”æ¡ˆ_output.md\"\n",
    "final_output = \"2026åˆ·é¢˜ç­_é¢˜ç›®ä¸ç­”æ¡ˆåˆå¹¶ç‰ˆ.md\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not Path(problem_file).exists() or not Path(answer_file).exists():\n",
    "        print(\"é”™è¯¯ï¼šè¯·ç¡®ä¿é¢˜ç›®æ–‡ä»¶å’Œç­”æ¡ˆæ–‡ä»¶éƒ½åœ¨å½“å‰ç›®å½•ä¸‹ã€‚\")\n",
    "    else:\n",
    "        merge_markdowns(problem_file, answer_file, final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
